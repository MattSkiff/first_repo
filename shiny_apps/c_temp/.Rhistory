blue <- drymatter.df$blue
green <- drymatter.df$green
nir <- drymatter.df$nir
drymatter <- drymatter.df$DryMatter
t.test(x = drymatter,y = drymatter2,paired = T)
drymatter2 <- drymatter.df$drymatter
t.test(x = drymatter,y = drymatter2,paired = T)
# STATS522: Assignment 3
#2. Fit another regression model (using some interactions) using rJAGS.
drymatter.df <-read.csv("G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\Dairy_NZ_merged_Estimates_2_days_Cleaned.csv")
red <- drymatter.df$red
blue <- drymatter.df$blue
green <- drymatter.df$green
nir <- drymatter.df$nir
drymatter <- drymatter.df$DryMatter
n <- length(drymatter)
drymatter.df <-read.csv("G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\Dairy_NZ_merged_Estimates_2_days_Cleaned.csv")
red <- drymatter.df$red
blue <- drymatter.df$blue
green <- drymatter.df$green
nir <- drymatter.df$nir
drymatter <- drymatter.df$DryMatter
n <- length(drymatter)
var <- c("Intercept","Red","Green","Blue","NIR","Green*Red","Green*Blue","Green*NIR","Red*Blue","Red*NIR","Blue*NIR","Sigma")
model = "model
{
# Likelihood
for(i in 1:n){
Y[i]   ~ dnorm(mu[i],inv.var)
# beta1 = intercept
mu[i] <- beta[1] + beta[2]*red[i] + beta[3]*blue[i] + beta[4]*green[i] + beta[5]*nir[i] +
# including all 2-way interactions
beta[6] * green[i] * red[i] +
beta[7] * green[i] * blue[i] +
beta[8] * green[i] * nir[i] +
beta[9] * red[i] * blue[i] +
beta[10] * red[i] * nir[i] +
beta[11] * blue[i] * nir[i]
}
# Prior for beta
for(j in 1:11){
beta[j] ~ dnorm(0,0.0001)
}
# Prior for the inverse variance
# sigma = residual standard error
inv.var   ~ dgamma(0.01, 0.01)
sigma     <- 1/sqrt(inv.var)
}
"
data = list(Y=drymatter,red=red, green=green,blue=blue,nir=nir,n=n)
# Variables to monitor
variable_names = c('beta','sigma')
# How many burn-in steps?
burn_in = 1000
# How many proper steps?
steps = 100000
# Thinning?
thin = 100
library(rjags)
# Write model out to file
fileConn=file("model.temp")
writeLines(model, fileConn)
close(fileConn)
if(all(is.na(data)))
{
m = jags.model(file="model.temp")
} else
{
m = jags.model(file="model.temp", data=data)
}
update(m, burn_in)
samp <- coda.samples(m, variable.names=c("beta","sigma"),
n.iter=steps, progress.bar="text",thin=thin)
summary(samp)
saveRDS(samp,"G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\samp_q2.rds")
#plot(samp)
# frequentist model for comparison
lm.mod <- lm(drymatter ~ red + blue + green + nir + green*red + green*blue + green*nir + red*blue + red*nir + blue*nir)
summary(lm.mod)
lm.mod <- lm(drymatter ~ red + blue + green + nir)
summary(lm.mod)
png('G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\autocorrelation_plots_model1.png')
var <- c("Intercept","Red","Green","Blue","NIR","Sigma")
par(mfrow = c(3,2))
for (i in 1:6) {
acf(samp[[1]][,i],main = paste("Auto-correlation of",var[i]))
}
dev.off()
png('G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\trace_plots_model1.png')
par(mfrow = c(3,2))
for (i in 1:6) {
plot(as.vector(samp[[1]][,i]),type = 'l',main = paste("Trace of ",var[i]),ylab = "Value",xlab = "Iteration")
}
dev.off()
# STATS522: Assignment 3
#1. Fit a linear regression model on DryMatter using blue, green, red and
#nir as covariates  using rJAGS.
## JAGS illustration for 2 sample test model ##
## sigma same for both
## using hyper-parameters
# linear regression on drymatter
# using RGB+NIR as covariates
# model adapted from https://www4.stat.ncsu.edu/~reich/st590/code/regJAGS
## Code adapted from Brendon Brewer - STATS331
## https://www.stat.auckland.ac.nz/~brewer/stats331.pdf
drymatter.df <-read.csv("G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\Dairy_NZ_merged_Estimates_2_days_Cleaned.csv")
red <- drymatter.df$red
blue <- drymatter.df$blue
green <- drymatter.df$green
nir <- drymatter.df$nir
drymatter <- drymatter.df$DryMatter
n <- length(drymatter)
model = "model
{
# Likelihood
for(i in 1:n){
Y[i]   ~ dnorm(mu[i],inv.var)
# beta1 = intercept
mu[i] <- beta[1] + beta[2]*red[i] + beta[3]*blue[i] + beta[4]*green[i] + beta[5]*nir[i]
}
# Prior for beta
for(j in 1:5){
beta[j] ~ dnorm(0,0.0001)
}
# Prior for the inverse variance
# sigma = residual standard error
inv.var   ~ dgamma(0.01, 0.01)
sigma     <- 1/sqrt(inv.var)
}
"
data = list(Y=drymatter,red=red, green=green,blue=blue,nir=nir,n=n)
# Variables to monitor
variable_names = c('beta','sigma')
# How many burn-in steps?
burn_in = 1000
# How many proper steps?
steps = 1000000
# Thinning?
thin = 1000
library(rjags)
# Write model out to file
fileConn=file("model.temp")
writeLines(model, fileConn)
close(fileConn)
if(all(is.na(data)))
{
m = jags.model(file="model.temp")
} else
{
m = jags.model(file="model.temp", data=data)
}
update(m, burn_in)
samp1 <- coda.samples(m, variable.names=c("beta","sigma"),
n.iter=steps, progress.bar="text",thin=thin)
summary(samp1)
#plot(samp)
saveRDS(samp1,"G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\samp_q1.rds")
# frequentist model for comparison
lm.mod <- lm(drymatter ~ red + blue + green + nir)
summary(lm.mod)
png('G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\autocorrelation_plots_model1.png')
var <- c("Intercept","Red","Green","Blue","NIR","Sigma")
par(mfrow = c(3,2))
for (i in 1:6) {
acf(samp1[[1]][,i],main = paste("Auto-correlation of",var[i]))
}
dev.off()
png('G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\trace_plots_model1.png')
par(mfrow = c(3,2))
for (i in 1:6) {
plot(as.vector(samp1[[1]][,i]),type = 'l',main = paste("Trace of ",var[i]),ylab = "Value",xlab = "Iteration")
}
dev.off()
plot_kde <- function(i) {
plot(density(as.vector(samp1[[1]][,i])),type = 'l',main = paste("KDE of ",var[i]),ylab = "Density",xlab = "Value")
polygon(density(as.vector(samp1[[1]][,i])),col = colr[i])
abline(col = "black",v = mean(as.vector(samp1[[1]][,i])))
abline(col = "black",v = median(as.vector(samp1[[1]][,i])),lty = 2)
}
png('G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\density_plots_model1.png')
colr <- c("grey","red","green","blue","orange","white")
m <- matrix(c(1,2,3,3,4,5,6,6,7,8),ncol = 2,byrow = TRUE)
layout(m,widths = c(0.5,0.5),heights = c(0.3,0.05,0.3,0.05,0.3))
par(mar = c(3,4,4,2) + 0.1,mgp=c(2,1,0))
plot_kde(1)
plot_kde(2)
par(mar = c(0,0,0,0))
plot(1,1,type = "n",frame.plot = FALSE,axes = FALSE)
u <- par("usr")
text(1,u[4],labels = "Black line indicates mean of posterior",col = "red",pos = 1)
par(mar = c(3,4,2,2) + 0.1,mgp=c(2,1,0))
plot_kde(3)
plot_kde(4)
par(mar = c(0,0,0,0))
plot(1,1,type = "n",frame.plot = FALSE,axes = FALSE)
u <- par("usr")
text(1,u[4],labels = "Dashed line indicates median of posterior",col = "red",pos = 1)
par(mar = c(3,4,2,2) + 0.1,mgp=c(2,1,0))
plot_kde(5)
plot_kde(6)
dev.off()
install.packages("BayesFactor")
# STATS522: Assignment 3
#2. Fit another regression model (using some interactions) using rJAGS.
drymatter.df <-read.csv("G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\Dairy_NZ_merged_Estimates_2_days_Cleaned.csv")
red <- drymatter.df$red
blue <- drymatter.df$blue
green <- drymatter.df$green
nir <- drymatter.df$nir
drymatter <- drymatter.df$DryMatter
n <- length(drymatter)
var <- c("Intercept","Red","Green","Blue","NIR","Green*Red","Green*Blue","Green*NIR","Red*Blue","Red*NIR","Blue*NIR","Sigma")
model = "model
{
# Likelihood
for(i in 1:n){
Y[i]   ~ dnorm(mu[i],inv.var)
# beta1 = intercept
mu[i] <- beta[1] + beta[2]*red[i] + beta[3]*blue[i] + beta[4]*green[i] + beta[5]*nir[i] +
# including all 2-way interactions
beta[6] * green[i] * red[i] +
beta[7] * green[i] * blue[i] +
beta[8] * green[i] * nir[i] +
beta[9] * red[i] * blue[i] +
beta[10] * red[i] * nir[i] +
beta[11] * blue[i] * nir[i]
}
# Prior for beta
for(j in 1:11){
beta[j] ~ dnorm(0,0.0001)
}
# Prior for the inverse variance
# sigma = residual standard error
inv.var   ~ dgamma(0.01, 0.01)
sigma     <- 1/sqrt(inv.var)
}
"
data = list(Y=drymatter,red=red, green=green,blue=blue,nir=nir,n=n)
# Variables to monitor
variable_names = c('beta','sigma')
# How many burn-in steps?
burn_in = 1000
# How many proper steps?
steps = 100000
# Thinning?
thin = 10000
library(rjags)
# Write model out to file
fileConn=file("model.temp")
writeLines(model, fileConn)
close(fileConn)
if(all(is.na(data)))
{
m = jags.model(file="model.temp")
} else
{
m = jags.model(file="model.temp", data=data)
}
update(m, burn_in)
samp <- coda.samples(m, variable.names=c("beta","sigma"),
n.iter=steps, progress.bar="text",thin=thin)
samp_bridge <- coda.samples(m, variable.names=c("beta","sigma"), parameters.to.save = c("beta", "inv.var"),
n.iter=steps, progress.bar="text",thin=thin)
summary(samp)
saveRDS(samp,"G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\samp_q2.rds")
saveRDS(samp_bridge,"G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\samp_q2_bridge.rds")
#plot(samp)
samp2 <- readRDS("G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\samp_q2.rds")
# frequentist model for comparison
lm_big.mod <- lm(drymatter ~ red + blue + green + nir + green*red + green*blue + green*nir + red*blue + red*nir + blue*nir)
summary(lm_big.mod)
## First six coefficients
png('G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\autocorrelation_plots_model2_1.png')
var <- c("Intercept","Red","Green","Blue","NIR","Sigma")
par(mfrow = c(3,2))
for (i in 1:6) {
acf(samp2[[1]][,i],main = paste("Auto-correlation of",var[i]))
}
dev.off()
png('G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\trace_plots_model2_1.png')
par(mfrow = c(3,2))
for (i in 1:6) {
plot(as.vector(samp2[[1]][,i]),type = 'l',main = paste("Trace of ",var[i]),ylab = "Value",xlab = "Iteration")
}
dev.off()
plot_kde <- function(i) {
plot(density(as.vector(samp2[[1]][,i])),type = 'l',main = paste("KDE of ",var[i]),ylab = "Density",xlab = "Value")
polygon(density(as.vector(samp2[[1]][,i])),col = colr[i])
abline(col = "black",v = mean(as.vector(samp2[[1]][,i])))
abline(col = "black",v = median(as.vector(samp2[[1]][,i])),lty = 2)
}
png('G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\density_plots_model2_1.png')
colr <- c("grey","red","green","blue","orange","white")
m <- matrix(c(1,2,3,3,4,5,6,6,7,8),ncol = 2,byrow = TRUE)
layout(m,widths = c(0.5,0.5),heights = c(0.3,0.05,0.3,0.05,0.3))
par(mar = c(3,4,4,2) + 0.1,mgp=c(2,1,0))
plot_kde(1)
plot_kde(2)
par(mar = c(0,0,0,0))
plot(1,1,type = "n",frame.plot = FALSE,axes = FALSE)
u <- par("usr")
text(1,u[4],labels = "Black line indicates mean of posterior",col = "red",pos = 1)
par(mar = c(3,4,2,2) + 0.1,mgp=c(2,1,0))
plot_kde(3)
plot_kde(4)
par(mar = c(0,0,0,0))
plot(1,1,type = "n",frame.plot = FALSE,axes = FALSE)
u <- par("usr")
text(1,u[4],labels = "Dashed line indicates median of posterior",col = "red",pos = 1)
par(mar = c(3,4,2,2) + 0.1,mgp=c(2,1,0))
plot_kde(5)
plot_kde(6)
dev.off()
## Last 5 coefficients
png('G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\autocorrelation_plots_model2_2.png')
var <- c("Intercept","Red","Green","Blue","NIR","Sigma")
par(mfrow = c(3,2))
for (i in 1:5) {
acf(samp2[[1]][,i+6],main = paste("Auto-correlation of",var[i+6]))
}
dev.off()
png('G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\trace_plots_model2_2.png')
par(mfrow = c(3,2))
for (i in 1:6) {
plot(as.vector(samp2[[1]][,i+6]),type = 'l',main = paste("Trace of ",var[i+6]),ylab = "Value",xlab = "Iteration")
}
dev.off()
plot_kde <- function(i) {
plot(density(as.vector(samp2[[1]][,i+6])),type = 'l',main = paste("KDE of ",var[i]),ylab = "Density",xlab = "Value")
polygon(density(as.vector(samp2[[1]][,i+6])),col = colr[i])
abline(col = "black",v = mean(as.vector(samp2[[1]][,i+6])))
abline(col = "black",v = median(as.vector(samp2[[1]][,i+6])),lty = 2)
}
png('G:\\My Drive\\2019 B Semester\\STATS522\\assignment3\\density_plots_model2_2.png')
colr <- c("grey","red","green","blue","orange","white")
m <- matrix(c(1,2,3,3,4,5,6,6,7,8),ncol = 2,byrow = TRUE)
layout(m,widths = c(0.5,0.5),heights = c(0.3,0.05,0.3,0.05,0.3))
par(mar = c(3,4,4,2) + 0.1,mgp=c(2,1,0))
plot_kde(1)
plot_kde(2)
par(mar = c(0,0,0,0))
plot(1,1,type = "n",frame.plot = FALSE,axes = FALSE)
u <- par("usr")
text(1,u[4],labels = "Black line indicates mean of posterior",col = "red",pos = 1)
par(mar = c(3,4,2,2) + 0.1,mgp=c(2,1,0))
plot_kde(3)
plot_kde(4)
par(mar = c(0,0,0,0))
plot(1,1,type = "n",frame.plot = FALSE,axes = FALSE)
u <- par("usr")
text(1,u[4],labels = "Dashed line indicates median of posterior",col = "red",pos = 1)
par(mar = c(3,4,2,2) + 0.1,mgp=c(2,1,0))
plot_kde(5)
plot_kde(6)
dev.off()
read.csv(file.choose())
images_list <- read.csv(file.choose())
file.choose()
images_list <- read.csv("C:\\Users\\user\\Desktop\\PetImages\\train\\flist.csv",row.names = F)
images_list <- read.csv("C:\\Users\\user\\Desktop\\PetImages\\train\\flist.csv",row.names = FALSE)
images_list <- read.csv("C:\\Users\\user\\Desktop\\PetImages\\train\\flist.csv")
images_list
rownames(images_list) <- NULL
rownames(images_list) <- NULL
images_list
images_list <- read.csv("C:\\Users\\user\\Desktop\\PetImages\\train\\flist.csv",header = F)
images_list
images_list
images_list[5000:5005,]
grep(images_list$V1)
grep(pattern = "1_",x = images_list$V1)
library(stringr)
str_detect(images_list$V1,"1_")
sum(str_detect(images_list$V1,"1_"))
length(images_list$V1)
as.integer(str_detect(images_list$V1,"1_"))
image_list$label <-as.integer(str_detect(images_list$V1,"1_"))
images_list$label <-as.integer(str_detect(images_list$V1,"1_"))
images_list
library(stringr)
images_list <- read.csv("C:\\Users\\user\\Desktop\\PetImages\\train\\flist.csv",header = F)
rownames(images_list) <- NULL
images_list$label <-as.integer(str_detect(images_list$V1,"1_"))
write.csv(export.csv,"flist_label.csv")
write.csv(images_list,"flist_label.csv")
library(stringr)
images_list <- read.csv("C:\\Users\\user\\Desktop\\PetImages\\train\\flist.csv",header = F)
rownames(images_list) <- NULL
images_list$label <-as.integer(str_detect(images_list$V1,"1_"))
write.csv(images_list,"C:\\Users\\user\\Desktop\\PetImages\\train\\flist_label.csv")
library(stringr)
images_list <- read.csv("C:\\Users\\user\\Desktop\\PetImages\\train\\flist.csv",header = F)
rownames(images_list) <- c()
images_list$label <-as.integer(str_detect(images_list$V1,"1_"))
images_list
write.csv(images_list,"C:\\Users\\user\\Desktop\\PetImages\\train\\flist_label.csv")
rownames(images_list)
write.csv(images_list,"C:\\Users\\user\\Desktop\\PetImages\\train\\flist_label.csv", row.names = FALSE)
file.open()
file.select()
file.list()
cast()
recast(0)
?recast
??recast
library(reshape2)
cast
recast
?rrecast
?recast
pct.na
cbind(c(1,2,3),c(1,2,3))
data.frame(c(1,2,3))
?match
install.packages("devtools")
# Gradient descent example
# Start with some function f(x) we wish to optimise
# This might be the log likelihood - which can often be optimised analytically
# Or using more complex methods (e.g. the hessian, newton-raphson, fisher info)
# In practice this will usually be the loss likelihood of a NN
# To make this little script more generalisation, will use an AD package
devtools::install_github("mailund/dfdr")
library(dfdr)
simple_GD.func <- function (objective_function = function(x) x^2 - 2*x, learning_rate = 0.01, initial_guess_min = -100, initial_guess_max = 100, plot = T,plot_window = 100) {
# NOTE: for dfdr to work, function must be expressed on single line (i.e. without brackets)
# e.g. eq.func <- function(x) x^2 - 2*x
require(dfdr)
# check arguments
if (!is.atomic(learning_rate) | !is.numeric(learning_rate)) {
stop("Learning rate must be scalar and numeric")
}
if (!is.atomic(initial_guess_min) | !is.numeric(initial_guess_min)) {
stop("Initial guess min must be scalar and numeric")
}
if (!is.atomic(initial_guess_max) | !is.numeric(initial_guess_max)) {
stop("Initial guess max must be scalar and numeric")
}
#if (!is.atomic(lower_bound)) {
#  stop("Min must be scalar")
#}
#if (!is.atomic(upper_bound)) {
#  stop("Max must be scalar")
#}
# Step 1: Initialise param - for simple function, will be a scalar
# Initial guess
theta.vec <- c()
theta.vec[1] <-  runif(n = 1,min = initial_guess_min,max = initial_guess_max)
# Step 2: Set learning rate - completely arbitrary at this stage
# Positive
if (abs(learning_rate) >= 1 | abs(learning_rate) == 0) {
message("Constant learning rate must be less than 1 (and positive) and non-zero - setting learning rate to 0.999")
eta.num <- 0.999
} else {
eta.num <- abs(learning_rate)
}
# Partial derivative with respect to x
# skipped warning and finally argument in try catch
f <- tryCatch({
d(objective_function,"x")
}, error = function(e) {
message("Function not differentiable via dfdr")
message("Original warning:")
stop(e)
})
theta_diff <- 10
i <- 2
# Looping until convergence
while (theta_diff/eta.num > 0.01) {
# updating param
theta.vec[i] <- theta.vec[i-1] - eta.num*f(theta.vec[i-1])
theta_diff <- abs(theta.vec[i]-theta.vec[i-1])
i <- i + 1
if (is.na(theta_diff)) {
stop("GD diverged")
}
if (i > 10^8) {
stop("Gradient Descent was unable to converge")
}
}
# plotting function
theta_last <- theta.vec[length(theta.vec)]
if (plot) {
curve(objective_function,
from = theta_last-plot_window,
to = theta_last+plot_window,
xlab = "theta",ylab = "f(theta)",
main = "Gradient Descent of Objective Function",
ylim = c(objective_function(theta_last)-plot_window,objective_function(theta_last)+plot_window),
sub = "theta values")
mtext(body(objective_function))
points(x = theta.vec,y = objective_function(theta.vec),col = 'red')
}
return(list(theta.vec = theta.vec,theta_last = theta_last,no_iterations = i))
}
# illustrative examples
func<- function(x) x^2 - 2*x
curve(func,to = 10,from = -10)
# converges
simple_GD.func()
# local minima
func<- function(x) ((x^2 + 1)/x)
curve(func,to = 10,from = -10)
# will time out if initial guess less than 0 - no minima below 0
simple_GD.func(func)
# ad library can't handle below function
#func<- function(x) (abs(x-6))
#curve(func,to = 10,from = -10)
#simple_GD.func(func)
simple_GD.func(func)
shiny::runApp('covid_nz')
runApp('covid_nz')
shiny::runApp('G:/My Drive/dev_working_folder/shiny_apps/c_temp')
